{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cb5f36",
   "metadata": {},
   "source": [
    "## Acquire and Analyze: Goodreads vs. NYT's Book Reviews\n",
    "\n",
    "In this notebook, I use three datasets - Wikipedia's list of books that have sold at least 10M copies, Goodreads community's top 100 rated books, and New York Times books review - to develop a list of books that have the most popular appeal while also being \"worthy\" of critical review.\n",
    "\n",
    "Then I pull some basic statistics on the text from the New York Times reviews of the books that were published after 2000 and meet all three criteria above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b73474",
   "metadata": {},
   "source": [
    "### Part 1: Analysis of Books on Top Sellers List, Goodreads, and NYTs Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcfc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import Comment\n",
    "import re\n",
    "from time import sleep\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/austinsmith/documents/Fall21/TextMining/NLTK\")\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d12d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in Wikipedia list of top selling books of all time \n",
    "\n",
    "top_sellers = open('Wikipedia_Best_Sellers','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ff94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garner a list of the top 100 books from the Goodreads Best Books list\n",
    "\n",
    "site = [\"https://www.goodreads.com/list/show/1.Best_Books_Ever\"]\n",
    "r = requests.get(site[0])\n",
    "r.status_code\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "book_titles = []\n",
    "\n",
    "for title in soup.find_all('a', class_ = 'bookTitle'):\n",
    "    book_titles.append(title.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348ba12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/book/show/2767052-the-hunger-games',\n",
       " '/book/show/2.Harry_Potter_and_the_Order_of_the_Phoenix',\n",
       " '/book/show/2657.To_Kill_a_Mockingbird',\n",
       " '/book/show/1885.Pride_and_Prejudice',\n",
       " '/book/show/41865.Twilight']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa249bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the book titles (using same method as enumerated in the data set shares notebook)\n",
    "titles = []\n",
    "\n",
    "for item in book_titles :\n",
    "    if \"/\" in item :\n",
    "        titles.append(item.split(\"/\"))\n",
    "\n",
    "\n",
    "clean_titles = []\n",
    "\n",
    "for item in titles:\n",
    "    clean_titles.append(item[-1]) \n",
    "    \n",
    "\n",
    "clean_titles_2 = []\n",
    "\n",
    "for item in clean_titles :\n",
    "    if '1984' not in item:\n",
    "        clean_titles_2.append(''.join([i for i in item if not i.isdigit()]))\n",
    "    else :\n",
    "        clean_titles_2.append(item)\n",
    "        \n",
    "\n",
    "titles_spaces = []\n",
    "\n",
    "for item in clean_titles_2 :\n",
    "    if '_' in item:\n",
    "        titles_spaces.append(item.replace('_',' '))\n",
    "    else :\n",
    "        titles_spaces.append(item)\n",
    "        \n",
    "\n",
    "titles_hyphens = []\n",
    "\n",
    "for item in titles_spaces :\n",
    "    if '-' in item:\n",
    "        titles_hyphens.append(item.replace('-',' '))\n",
    "    else :\n",
    "        titles_hyphens.append(item)\n",
    "        \n",
    "        \n",
    "\n",
    "fix_1984 = []\n",
    "\n",
    "for item in titles_hyphens :\n",
    "    if item == '40961427 1984' :\n",
    "        fix_1984.append(item.replace('40961427 1984',' 1984'))\n",
    "    else :\n",
    "        fix_1984.append(item)\n",
    "        \n",
    "        \n",
    "        \n",
    "goodreads_titles = []\n",
    "\n",
    "for item in fix_1984:\n",
    "    goodreads_titles.append(item[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a224296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the hunger games',\n",
       " 'Harry Potter and the Order of the Phoenix',\n",
       " 'To Kill a Mockingbird',\n",
       " 'Pride and Prejudice',\n",
       " 'Twilight',\n",
       " 'The Book Thief',\n",
       " 'Animal Farm',\n",
       " 'The Chronicles of Narnia',\n",
       " 'J R R Tolkien  Book Boxed Set',\n",
       " 'the fault in our stars']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodreads_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05578b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many titles in the Goodreads list are also\n",
    "# on the list of top-selling books from Wikipedia\n",
    "\n",
    "best_sellers_goodreads = []\n",
    "\n",
    "for item in goodreads_titles :\n",
    "    if item in top_sellers :\n",
    "        best_sellers_goodreads.append(item)\n",
    "    else :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18629b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_sellers_goodreads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf193c",
   "metadata": {},
   "source": [
    "In total, 26 of the 100 books on Goodread's Best Books list sold at least 10 million copies worldwide. \n",
    "\n",
    "Now I will see how many of those books were also reviewed by the New York Times. I know (from my data set shares notebook), that there are 31 books on Goodreads 100 Best Books list that were also reviewed by the NYT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d92b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert list of NYT/Goodreads overlap from the data shares jupyter notebook\n",
    "# (note: I just copied and pasted here)\n",
    "\n",
    "nyt_goodreads_overlap = (\n",
    "'To%20Kill%20a%20Mockingbird',\n",
    " 'Twilight',\n",
    " 'The%20Book%20Thief',\n",
    " 'the%20fault%20in%20our%20stars',\n",
    " 'The%20Da%20Vinci%20Code',\n",
    " 'Memoirs%20of%20a%20Geisha',\n",
    " 'divergent',\n",
    " 'Crime%20and%20Punishment',\n",
    " 'The%20Little%20Prince',\n",
    " 'City%20of%20Bones',\n",
    " 'the%20help',\n",
    " 'Brave%20New%20World',\n",
    " 'A%20Thousand%20Splendid%20Suns',\n",
    " 'the%20lovely%20bones',\n",
    " 'The%20Odyssey',\n",
    " 'Life%20of%20Pi',\n",
    " 'Water%20for%20Elephants',\n",
    " 'The%20Handmaid%27s%20Tale',\n",
    " 'dune',\n",
    " 'Little%20Women',\n",
    " 'Harry%20Potter%20and%20the%20Deathly%20Hallows',\n",
    " 'The%20Stand',\n",
    " 'anna%20karenina',\n",
    " 'The%20Girl%20with%20the%20Dragon%20Tattoo',\n",
    " 'My%20Sister%27s%20Keeper',\n",
    " 'the%20color%20purple',\n",
    " 'The%20Road',\n",
    " 'Angela%27s%20Ashes',\n",
    " 'Don%20Quixote',\n",
    " 'the%20notebook',\n",
    " 'A%20Prayer%20for%20Owen%20Meany')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71c7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the overlap list \n",
    "\n",
    "clean_overlap = []\n",
    "\n",
    "for item in nyt_goodreads_overlap :\n",
    "    if '%20' in item :\n",
    "        clean_overlap.append(item.replace('%20',' '))\n",
    "    else :\n",
    "        clean_overlap.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7446649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the Goodreads books that were reviewed by the NYTs to the Goodreads books \n",
    "# that sold at least 10M copies\n",
    "\n",
    "nyt_reviewed_bs = []\n",
    "\n",
    "for item in clean_overlap : \n",
    "    if item in best_sellers_goodreads :\n",
    "        nyt_reviewed_bs.append(item)\n",
    "    else :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0013e53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To Kill a Mockingbird',\n",
       " 'The Book Thief',\n",
       " 'The Da Vinci Code',\n",
       " 'The Little Prince',\n",
       " 'Life of Pi',\n",
       " 'Harry Potter and the Deathly Hallows',\n",
       " 'The Girl with the Dragon Tattoo']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_reviewed_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52614d6",
   "metadata": {},
   "source": [
    "In total, only 7 books were 1) voted one of the top 100 best books by the Goodreads community, 2) reviewed by the NYT, and 3) sold at least 10M copies. Those books - which both appeal to the masses and are worthy of critical review - include the following: \n",
    "- To Kill a Mockingbird\n",
    "- The Book Thief\n",
    "- The Da Vinci Code\n",
    "- The Little Prince\n",
    "- Life of Pi\n",
    "- Harry Potter and the Deathly Hallows\n",
    "- The Girl with the Dragon Tattoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ffda6",
   "metadata": {},
   "source": [
    "### Part 2: Text Analysis of NYT Review of Most Popular Books\n",
    "\n",
    "I use the information from the Wikipedia dataset to garner a bit more information on each of the top books identified in part 1 (please see the Wikipedia notebook in the Data Set Shares repo for this data source). \n",
    "\n",
    "The Little Prince sold the most copies (100M copies), followed by the Da Vinci Code, Harry Potter and the Deathly Hallows, To Kill a Mockingbird, The Girl with the Dragon Tattoo, The Book Thief, and Life of Pi (10M copies).\n",
    "\n",
    "All books but two were originally printed in English; The Little Prince was printed in French and The Girl with the Dragon Tattoo was published in Swedish. \n",
    "\n",
    "Five books were published between 2001 and 2007. The Little Prince was published in 1943 and To Kill a Mockingbird was published in 1960. \n",
    "\n",
    "For my text analysis, I will analyze the four books that were published after 2000, as that will give us more of a \"modern-day\" interpretation of the literature by the critics. Those books are (in order of most recently published):\n",
    "- Harry Potter and the Deathly Hallows\n",
    "- The Girl with the Dragon Tattoo\n",
    "- The Book Thief\n",
    "- The Da Vince Code\n",
    "- Life of Pi\n",
    "\n",
    "I will clean each text and report the following:\n",
    "- total number of tokens \n",
    "- unique tokens\n",
    "- average token length\n",
    "- lexical diversity\n",
    "- top ten words used in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040d20cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, read in all five bodies of text \n",
    "# The Harry Potter book has three reviews to read in; the Book Thief has two\n",
    "# The other books only have one review each\n",
    "\n",
    "# read in the easy files first \n",
    "\n",
    "life_of_pi = open('Life of Pi.txt','r').read()\n",
    "da_vinci_code = open('The Da Vinci Code.txt','r').read()\n",
    "dragon_tattoo = open('The Girl with the Dragon Tattoo.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f003b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the Harry Potter Reviews\n",
    "\n",
    "filenames = ['Harry Potter 1.txt', 'Harry Potter 2.txt', 'Harry Potter 3.txt']\n",
    "\n",
    "with open('Harry Potter.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "harry_potter = open('Harry Potter.txt','r').read()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8970e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in The Book Thief Reviews\n",
    "\n",
    "filenames = ['The Book Thief 1.txt', 'The Book Thief 2.txt']\n",
    "\n",
    "with open('The Book Thief.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "book_thief = open('The Book Thief.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6767fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the patterns function to pull each of these statistics\n",
    "\n",
    "def get_patterns(text,num_words):\n",
    "       \n",
    "    # clean the text (split on whitespace, change to lowercase, and drop non-alphanumerics and stopwords)\n",
    "    clean_text_split = [w.lower() for w in text.split()]\n",
    "    clean_text = [w for w in clean_text_split if w.isalpha and w not in sw]\n",
    "    \n",
    "    # report the total number of tokens\n",
    "    total_tokens = len(clean_text)\n",
    "    \n",
    "    # report the total number of unique tokens\n",
    "    unique_tokens = len(set(clean_text))\n",
    "    \n",
    "    # report the average token length\n",
    "    clean_text_token_len = [len(w) for w in clean_text]\n",
    "    avg_token_len = np.mean(clean_text_token_len)\n",
    "    \n",
    "    # report the lexical diversity\n",
    "    lex_diversity = len(set(clean_text))/len(clean_text)\n",
    "    \n",
    "    # report the top 10 most used tokens\n",
    "    clean_text_counter = Counter(clean_text)\n",
    "    top_10 = clean_text_counter.most_common(num_words)\n",
    "    \n",
    "    results = {'tokens':total_tokens,\n",
    "               'unique_tokens':unique_tokens,\n",
    "               'avg_token_length':avg_token_len,\n",
    "               'lexical_diversity':lex_diversity,\n",
    "               'top_10':top_10}\n",
    "    \n",
    "    return(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dd1f8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 858,\n",
       " 'unique_tokens': 684,\n",
       " 'avg_token_length': 6.462703962703963,\n",
       " 'lexical_diversity': 0.7972027972027972,\n",
       " 'top_10': [('story', 9),\n",
       "  ('pi', 9),\n",
       "  ('new', 9),\n",
       "  ('--', 9),\n",
       "  ('tiger', 6),\n",
       "  ('one', 6),\n",
       "  ('book', 6),\n",
       "  ('reading', 5),\n",
       "  ('books', 4),\n",
       "  ('animal', 4)]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(life_of_pi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de14e52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 874,\n",
       " 'unique_tokens': 690,\n",
       " 'avg_token_length': 6.821510297482837,\n",
       " 'lexical_diversity': 0.7894736842105263,\n",
       " 'top_10': [('da', 11),\n",
       "  ('vinci', 11),\n",
       "  ('holy', 9),\n",
       "  ('new', 9),\n",
       "  ('books', 7),\n",
       "  (\"''holy\", 7),\n",
       "  ('blood,', 7),\n",
       "  (\"''the\", 6),\n",
       "  ('one', 6),\n",
       "  ('book', 6)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(da_vinci_code,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b8d330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 529,\n",
       " 'unique_tokens': 431,\n",
       " 'avg_token_length': 6.546313799621928,\n",
       " 'lexical_diversity': 0.8147448015122873,\n",
       " 'top_10': [('book', 5),\n",
       "  ('swedish', 5),\n",
       "  ('blomkvist', 5),\n",
       "  ('“girl”', 5),\n",
       "  ('reading', 4),\n",
       "  ('story', 4),\n",
       "  ('henrik', 4),\n",
       "  ('.', 4),\n",
       "  ('site', 3),\n",
       "  ('vanished', 3)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(dragon_tattoo,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed62ea2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 2349,\n",
       " 'unique_tokens': 1525,\n",
       " 'avg_token_length': 6.558535547041294,\n",
       " 'lexical_diversity': 0.6492124308216263,\n",
       " 'top_10': [('—', 30),\n",
       "  ('story', 17),\n",
       "  ('reading', 16),\n",
       "  ('harry', 16),\n",
       "  ('book', 13),\n",
       "  ('.', 13),\n",
       "  ('new', 12),\n",
       "  ('main', 11),\n",
       "  ('us', 11),\n",
       "  ('continue', 10)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(harry_potter,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0725e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 1573,\n",
       " 'unique_tokens': 1017,\n",
       " 'avg_token_length': 6.341385886840432,\n",
       " 'lexical_diversity': 0.6465352828989193,\n",
       " 'top_10': [('book', 36),\n",
       "  ('liesel', 20),\n",
       "  ('\"the', 15),\n",
       "  ('new', 13),\n",
       "  ('books', 13),\n",
       "  ('death', 12),\n",
       "  ('reading', 10),\n",
       "  ('thief\"', 10),\n",
       "  ('story', 9),\n",
       "  ('max', 9)]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_patterns(book_thief,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213d4ac",
   "metadata": {},
   "source": [
    "Most of the book reviews have roughly 500-800 words, which makes sense seeing as they're all written for the same magazine (keep in mind that the Book Thief contains a total of two reviews and the Harry Potter books has three; on average, their reviews are about 800 tokens each). \n",
    "\n",
    "The average token length is about 6.5 for all books and the lexical diversity ranges from about 0.65 to 0.81, which indicates that all of the reviews are written with roughly the same level of complexity, regardless of what audience the book is intended to entertain (e.g., teens vs. adults). \n",
    "\n",
    "The most used words are predictable for several of the books. For example, it makes sense that \"pi\" and \"tiger\" would be among the top 10 words for Life of Pi. It also make sense that \"holy\" and \"blood\" would show up in The Da Vinci Code and that Harry Potter reviews would mention \"Harry\" frequently. \n",
    "\n",
    "There are some unexpected words, though. I am surprised to see that the review of The Girl with the Dragon Tattoo mentions Henrik and Blomkvist, male characters in the story, multiple times, but Lisbeth, the name of the female protagonist, does not appear in the top 10 words. Since \"Swedish\" is also in the top 10 words, the review may have been focused more on the role of men in Swedish society, or how they are portrayed in the book, rather than on the development of the main character. \n",
    "\n",
    "This was not the case for The Book Thief, where the main protagonist - Liesel - is mentioned often. Her mentions far outweigh the second main character, Max, which indicates that review may have focused on Liesel as its primary subject. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
